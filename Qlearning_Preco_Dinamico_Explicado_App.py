
import streamlit as st
import numpy as np
import pandas as pd

st.title("üí∞ Pre√ßo Din√¢mico com Q-Learning ‚Äî Simula√ß√£o no E-commerce")

st.markdown("""
## üìå Case: Otimiza√ß√£o de Pre√ßo Din√¢mico no E-commerce

Voc√™ √© um **consultor de IA** contratado por um e-commerce.  
Esse e-commerce quer **maximizar o lucro** ajustando dinamicamente o pre√ßo de um produto ao longo do tempo, em diferentes situa√ß√µes de demanda.

‚û° **Desafio:** Descobrir qual pre√ßo aplicar em cada cen√°rio de demanda (alta, m√©dia, baixa) para obter o maior lucro poss√≠vel.  
‚û° **Solu√ß√£o proposta:** Usar o **Q-Learning** para o agente aprender, por tentativa e erro, qual pre√ßo escolher em cada situa√ß√£o.

üí° **No final, o aluno deve interpretar a Q-table e explicar como o e-commerce poderia usar essa pol√≠tica para definir seus pre√ßos reais.**
""")

st.markdown("""
### üîß Hiperpar√¢metros do Q-Learning
- **Taxa de aprendizado (Œ±)**: controla o quanto o agente aprende com experi√™ncias novas.  
  - Valor alto: aprende r√°pido, mas esquece r√°pido.  
  - Valor baixo: aprende devagar, mas de forma est√°vel.
- **Fator de desconto (Œ≥)**: define o peso das recompensas futuras.  
  - Valor alto: pensa no longo prazo.  
  - Valor baixo: valoriza o ganho imediato.
- **Taxa de explora√ß√£o (Œµ)**: controla o quanto o agente explora a√ß√µes novas ao inv√©s de usar o que j√° sabe.  
  - Valor alto: explora mais no come√ßo.
- **N√∫mero de epis√≥dios**: quanto mais epis√≥dios, mais o agente tem chance de aprender.
""")

# Configura√ß√µes do ambiente
states = ["Alta Demanda", "M√©dia Demanda", "Baixa Demanda"]
actions = ["Pre√ßo Baixo", "Pre√ßo M√©dio", "Pre√ßo Alto"]
n_states = len(states)
n_actions = len(actions)

# Inicializar Q-Table
Q = np.zeros((n_states, n_actions))

# Hiperpar√¢metros ajust√°veis pelo aluno
alpha = st.slider("Taxa de aprendizado (Œ±)", 0.01, 1.0, 0.1)
gamma = st.slider("Fator de desconto (Œ≥)", 0.01, 1.0, 0.9)
epsilon = st.slider("Taxa de explora√ß√£o (Œµ)", 0.0, 1.0, 0.2)
episodios = st.slider("N√∫mero de epis√≥dios de treino", 100, 5000, 1000)

# Fun√ß√£o de recompensa
def obter_recompensa(state, action):
    tabela_recompensa = {
        (0, 0): 8, (0, 1): 10, (0, 2): 12,  # Alta demanda
        (1, 0): 5, (1, 1): 8, (1, 2): 6,   # M√©dia demanda
        (2, 0): 3, (2, 1): 4, (2, 2): 2    # Baixa demanda
    }
    return tabela_recompensa[(state, action)]

# Treinamento
for _ in range(episodios):
    state = np.random.randint(0, n_states)
    if np.random.rand() < epsilon:
        action = np.random.randint(0, n_actions)
    else:
        action = np.argmax(Q[state])

    reward = obter_recompensa(state, action)
    next_state = np.random.randint(0, n_states)

    Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])

# Mostrar Q-Table e pol√≠tica aprendida
df_q = pd.DataFrame(Q, index=states, columns=actions)
st.subheader("Q-Table final aprendida")
st.dataframe(df_q.style.format("{:.2f}"))

# Melhor a√ß√£o por estado
policy = df_q.idxmax(axis=1).rename("Melhor Pre√ßo Sugerido")
st.subheader("Pol√≠tica aprendida (Melhor pre√ßo por n√≠vel de demanda)")
st.dataframe(policy)

st.markdown("""
### üìå Como usar a Q-Table no e-commerce?
- A Q-Table mostra o valor esperado de lucro para cada a√ß√£o em cada situa√ß√£o de demanda.
- O e-commerce poderia usar a **melhor a√ß√£o sugerida** (pre√ßo) como orienta√ß√£o para seu sistema de precifica√ß√£o autom√°tica.
- Exemplo: Se a demanda est√° alta ‚Üí o sistema aplica o pre√ßo alto, porque foi o que o agente aprendeu que maximiza o lucro.

### üéØ Atividade para o aluno
‚úÖ Execute o treinamento e observe a pol√≠tica aprendida.  
‚úÖ Explique:
- Qual pre√ßo o agente recomenda para cada n√≠vel de demanda?
- Por que essa pol√≠tica faz sentido (ou n√£o) para o e-commerce?
- Como voc√™ ajustaria os par√¢metros para obter uma pol√≠tica diferente?

üí° **Dica:** Teste diferentes valores de Œµ, Œ± e Œ≥ para entender o impacto no aprendizado!
""")
